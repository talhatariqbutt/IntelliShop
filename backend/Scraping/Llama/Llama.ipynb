{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "\n",
    "def get_product_info(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    products = []\n",
    "\n",
    "    for product in soup.find_all('div', class_='grid-product'):\n",
    "        \n",
    "        product_info = {}\n",
    "\n",
    "        product_url = 'https://lamaretail.com' + product.find('a')['href']\n",
    "\n",
    "        name_elem = product.find('div', class_='grid-product__title--body')\n",
    "        name = name_elem.text.strip() if name_elem else \"\"\n",
    "        \n",
    "        price_elem = product.find('span', class_='money')\n",
    "        price = price_elem.text.strip() if price_elem else \"\"\n",
    "        \n",
    "        # Extracting name from the URL\n",
    "        name_match = re.search(r'products/([^\\/]+)', product_url)\n",
    "        name = name_match.group(1).replace('-', ' ').title() if name_match else \"\"\n",
    "        \n",
    "        # Extracting color from the URL\n",
    "        color_match = re.search(r'products/([^\\/]+)', product_url)\n",
    "        color = color_match.group(1).replace('-', ' ').title() if color_match else \"\"\n",
    "        \n",
    "        sizes_elem = product.find('div', class_='product-option-container')\n",
    "        sizes = re.findall(r'\\b[XSMLXL]+\\b', sizes_elem.text.strip()) if sizes_elem else []\n",
    "        \n",
    "\n",
    "        product_info['name'] = name\n",
    "        product_info['price'] = price\n",
    "        product_info['color'] = color\n",
    "        product_info['sizes'] = sizes\n",
    "        product_info['url'] = product_url\n",
    "\n",
    "        products.append(product_info)\n",
    "\n",
    "    return products\n",
    "\n",
    "def scrape_and_save(urls, filename='lama2.json'):\n",
    "    all_products = []\n",
    "    for url in urls:\n",
    "        page_number = 1\n",
    "        while True:\n",
    "            page_url = f\"{url}?page={page_number}\"\n",
    "            products = get_product_info(page_url)\n",
    "            if not products:\n",
    "                break\n",
    "            all_products.extend(products)\n",
    "            page_number += 1\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(all_products, f, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    urls = [\n",
    "        \"https://lamaretail.com/collections/man-t-shirts\",\n",
    "        \"https://lamaretail.com/collections/man-sweaters-cardigans\",\n",
    "        \"https://lamaretail.com/collections/man-shoes\",\n",
    "        \"https://lamaretail.com/collections/man-pants\",\n",
    "        \"https://lamaretail.com/collections/man-blazers\",\n",
    "        \"https://lamaretail.com/collections/man-polo\",\n",
    "        \"https://lamaretail.com/collections/man-shirts\",\n",
    "        \"https://lamaretail.com/collections/man-jackets-coats\",\n",
    "        \"https://lamaretail.com/collections/man-hoodies-sweatshirt\"\n",
    "    ]\n",
    "\n",
    "    scrape_and_save(urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "\n",
    "def get_product_info(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    products = []\n",
    "\n",
    "    for product in soup.find_all('div', class_='grid-product'):\n",
    "        \n",
    "        product_info = {}\n",
    "\n",
    "        product_url = 'https://lamaretail.com' + product.find('a')['href']\n",
    "\n",
    "        name_elem = product.find('div', class_='grid-product__title--body')\n",
    "        name = name_elem.text.strip() if name_elem else \"\"\n",
    "        \n",
    "        price_elem = product.find('span', class_='money')\n",
    "        price = price_elem.text.strip() if price_elem else \"\"\n",
    "        \n",
    "        # Extracting name from the URL\n",
    "        name_match = re.search(r'/products/([^\\/]+)', product_url)\n",
    "        name = name_match.group(1).replace('-', ' ').title() if name_match else \"\"\n",
    "        \n",
    "        # Extracting color from the URL\n",
    "        color_match = re.search(r'-(\\w+)$', product_url)\n",
    "        color = color_match.group(1).replace('-', ' ').title() if color_match else \"\"\n",
    "        \n",
    "        sizes_elem = product.find('div', class_='product-option-container')\n",
    "        sizes = re.findall(r'\\b[XSMLXL]+\\b', sizes_elem.text.strip()) if sizes_elem else []\n",
    "\n",
    "        product_info['name'] = name\n",
    "        product_info['price'] = price\n",
    "        product_info['color'] = color\n",
    "        product_info['sizes'] = sizes\n",
    "        product_info['url'] = product_url\n",
    "\n",
    "        products.append(product_info)\n",
    "\n",
    "    return products\n",
    "\n",
    "def scrape_and_save(urls, filename='lama5.json'):\n",
    "    all_products = []\n",
    "    for url in urls:\n",
    "        page_number = 1\n",
    "        while True:\n",
    "            page_url = f\"{url}?page={page_number}\"\n",
    "            products = get_product_info(page_url)\n",
    "            if not products:\n",
    "                break\n",
    "            all_products.extend(products)\n",
    "            page_number += 1\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(all_products, f, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    urls = [\n",
    "        \"https://lamaretail.com/collections/man-t-shirts\",\n",
    "        \"https://lamaretail.com/collections/man-sweaters-cardigans\",\n",
    "        \"https://lamaretail.com/collections/man-shoes\",\n",
    "        \"https://lamaretail.com/collections/man-pants\",\n",
    "        \"https://lamaretail.com/collections/man-blazers\",\n",
    "        \"https://lamaretail.com/collections/man-polo\",\n",
    "        \"https://lamaretail.com/collections/man-shirts\",\n",
    "        \"https://lamaretail.com/collections/man-jackets-coats\",\n",
    "        \"https://lamaretail.com/collections/man-hoodies-sweatshirt\"\n",
    "    ]\n",
    "\n",
    "    scrape_and_save(urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "\n",
    "def get_product_info(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    products = []\n",
    "\n",
    "    for product in soup.find_all('div', class_='grid-product'):\n",
    "        \n",
    "        product_info = {}\n",
    "\n",
    "        product_url = 'https://lamaretail.com' + product.find('a')['href']\n",
    "\n",
    "        name_elem = product.find('div', class_='grid-product__title--body')\n",
    "        name = name_elem.text.strip() if name_elem else \"\"\n",
    "        \n",
    "        price_elem = product.find('span', class_='money')\n",
    "        price = price_elem.text.strip() if price_elem else \"\"\n",
    "        \n",
    "        # Extracting name from the URL\n",
    "        name_match = re.search(r'/products/([^\\/-]+)', product_url)\n",
    "        name = name_match.group(1).replace('-', ' ').title() if name_match else \"\"\n",
    "        \n",
    "        # Extracting color from the URL\n",
    "        color_match = re.search(r'-(\\w+)$', product_url)\n",
    "        color = color_match.group(1).replace('-', ' ').title() if color_match else \"\"\n",
    "        \n",
    "        sizes_elem = product.find('div', class_='product-option-container')\n",
    "        sizes = re.findall(r'\\b[XSMLXL]+\\b', sizes_elem.text.strip()) if sizes_elem else []\n",
    "\n",
    "        product_info['name'] = name\n",
    "        product_info['price'] = price\n",
    "        product_info['color'] = color\n",
    "        product_info['sizes'] = sizes\n",
    "        product_info['url'] = product_url\n",
    "\n",
    "        products.append(product_info)\n",
    "\n",
    "    return products\n",
    "\n",
    "def scrape_and_save(urls, filename='lama6.json'):\n",
    "    all_products = []\n",
    "    for url in urls:\n",
    "        page_number = 1\n",
    "        while True:\n",
    "            page_url = f\"{url}?page={page_number}\"\n",
    "            products = get_product_info(page_url)\n",
    "            if not products:\n",
    "                break\n",
    "            all_products.extend(products)\n",
    "            page_number += 1\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(all_products, f, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    urls = [\n",
    "        \"https://lamaretail.com/collections/man-t-shirts\",\n",
    "        \"https://lamaretail.com/collections/man-sweaters-cardigans\",\n",
    "        \"https://lamaretail.com/collections/man-shoes\",\n",
    "        \"https://lamaretail.com/collections/man-pants\",\n",
    "        \"https://lamaretail.com/collections/man-blazers\",\n",
    "        \"https://lamaretail.com/collections/man-polo\",\n",
    "        \"https://lamaretail.com/collections/man-shirts\",\n",
    "        \"https://lamaretail.com/collections/man-jackets-coats\",\n",
    "        \"https://lamaretail.com/collections/man-hoodies-sweatshirt\"\n",
    "    ]\n",
    "\n",
    "    scrape_and_save(urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 223 images. Data saved to LamaRetail_images.json.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# Update the URL list as needed\n",
    "urls = [\n",
    "    \"https://lamaretail.com/collections/man-t-shirts\",\n",
    "    \"https://lamaretail.com/collections/man-sweaters-cardigans\",\n",
    "    \"https://lamaretail.com/collections/man-shoes\",\n",
    "    \"https://lamaretail.com/collections/man-pants\",\n",
    "    \"https://lamaretail.com/collections/man-blazers\",\n",
    "    \"https://lamaretail.com/collections/man-polo\",\n",
    "    \"https://lamaretail.com/collections/man-shirts\",\n",
    "    \"https://lamaretail.com/collections/man-jackets-coats\",\n",
    "    \"https://lamaretail.com/collections/man-hoodies-sweatshirt\"\n",
    "]\n",
    "\n",
    "products_data = []\n",
    "\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    products = soup.select('div.grid-product__image-mask')\n",
    "    \n",
    "    for product in products:\n",
    "        img_tag = product.find('img')\n",
    "        \n",
    "        if img_tag and 'src' in img_tag.attrs:\n",
    "            img_url = img_tag['src']\n",
    "            # Convert protocol-relative URL to absolute URL\n",
    "            if img_url.startswith(\"//\"):\n",
    "                img_url = \"https:\" + img_url\n",
    "            products_data.append({\"image_url\": img_url})\n",
    "\n",
    "with open('LamaRetail_images.json', 'w') as file:\n",
    "    json.dump(products_data, file, indent=4)\n",
    "\n",
    "print(f\"Extracted {len(products_data)} images. Data saved to LamaRetail_images.json.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded https://lamaretail.com/cdn/shop/files/MAS24TP054-OFF-WHITE-INTERLOCK-TEE_4.jpg?v=1717674496&width=1080 as LamaRetail_images/image_0.jpg\n",
      "Downloaded https://lamaretail.com/cdn/shop/files/MAW24TP134-BLACK-HEAVY-WEIGHT-TEE-000000_3.jpg?v=1726064926&width=1080 as LamaRetail_images/image_1.jpg\n",
      "Downloaded https://lamaretail.com/cdn/shop/files/MAS24TP067-NAVY-HEAVY-WEIGHT-TEE-282746_2.jpg?v=1724046445&width=1080 as LamaRetail_images/image_2.jpg\n",
      "Downloaded https://lamaretail.com/cdn/shop/files/MAS24TP067-IVORY-HEAVY-WEIGHT-TEE-bdaa99_11.jpg?v=1726064058&width=1080 as LamaRetail_images/image_3.jpg\n",
      "Downloaded https://lamaretail.com/cdn/shop/files/MAS24TP054-MAROON-INTERLOCK-TEE_3.jpg?v=1723202606&width=1080 as LamaRetail_images/image_4.jpg\n",
      "Downloaded https://lamaretail.com/cdn/shop/files/MAS24TP011-OLIVE-JACQUARD-TEE_1.jpg?v=1726228291&width=1080 as LamaRetail_images/image_5.jpg\n",
      "Downloaded https://lamaretail.com/cdn/shop/files/MAS24TP011-DARK-BROWN-JACQUARD-TEE-44302f_6.jpg?v=1726063856&width=1080 as LamaRetail_images/image_6.jpg\n",
      "Downloaded https://lamaretail.com/cdn/shop/files/MAS24TP068-WHITE-V-NECK-TEE_2_25f7bd21-e50a-4679-aad8-dd1739ca82a5.jpg?v=1721883815&width=1080 as LamaRetail_images/image_7.jpg\n",
      "Downloaded https://lamaretail.com/cdn/shop/files/MAS24TP068-OLIVE-V-NECK-TEE_5.jpg?v=1721650523&width=1080 as LamaRetail_images/image_8.jpg\n",
      "Downloaded https://lamaretail.com/cdn/shop/files/MAS24TP068-IVORY-V-NECK-TEE_1_66606663-9e83-4260-a0bb-3bbda7119d83.jpg?v=1721883828&width=1080 as LamaRetail_images/image_9.jpg\n",
      "Downloaded https://lamaretail.com/cdn/shop/files/MAS24TP068-BLACK-V-NECK-TEE_2_963ede49-3a8a-40ed-8ee5-17b8368a2337.jpg?v=1721883838&width=1080 as LamaRetail_images/image_10.jpg\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m image_url \u001b[38;5;241m=\u001b[39m product[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_url\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Get the image content\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()  \u001b[38;5;66;03m# Check if the request was successful\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Save the image to a file\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    790\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    809\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py:537\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 537\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/urllib3/connection.py:466\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    465\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1395\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1393\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1394\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1395\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1396\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1397\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:325\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 325\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:286\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 286\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1314\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1312\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1313\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1166\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Load the image URLs from the JSON file\n",
    "with open('LamaRetail_images.json', 'r') as file:\n",
    "    products_data = json.load(file)\n",
    "\n",
    "# Directory where images will be saved\n",
    "image_directory = 'LamaRetail_images'\n",
    "os.makedirs(image_directory, exist_ok=True)\n",
    "\n",
    "for idx, product in enumerate(products_data):\n",
    "    image_url = product['image_url']\n",
    "    try:\n",
    "        # Get the image content\n",
    "        response = requests.get(image_url)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        \n",
    "        # Save the image to a file\n",
    "        image_path = os.path.join(image_directory, f'image_{idx}.jpg')\n",
    "        with open(image_path, 'wb') as img_file:\n",
    "            img_file.write(response.content)\n",
    "        \n",
    "        print(f\"Downloaded {image_url} as {image_path}\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Failed to download {image_url}: {e}\")\n",
    "\n",
    "print(\"All available images have been downloaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "import time\n",
    "\n",
    "# Headers to mimic a browser request\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Base URL for product pages\n",
    "base_url = \"https://lamaretail.com\"\n",
    "\n",
    "# Directory to save images\n",
    "image_directory = 'LamaRetail_images'\n",
    "os.makedirs(image_directory, exist_ok=True)\n",
    "\n",
    "# CSV file setup\n",
    "csv_file_path = 'lama_retail_products.csv'\n",
    "\n",
    "def download_image(image_url, image_directory, image_id):\n",
    "    \"\"\" Download image and save locally \"\"\"\n",
    "    response = requests.get(image_url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        image_path = os.path.join(image_directory, f'{image_id}.jpg')\n",
    "        with open(image_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        return image_id, image_url\n",
    "    return None, None\n",
    "\n",
    "def get_product_info(url):\n",
    "    \"\"\" Extract product details from a given URL \"\"\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    products = []\n",
    "\n",
    "    for product in soup.find_all('div', class_='grid-product'):\n",
    "        product_url = base_url + product.find('a')['href']\n",
    "        name = product.find('div', class_='grid-product__title--body').text.strip() if product.find('div', class_='grid-product__title--body') else 'N/A'\n",
    "        price = product.find('span', class_='money').text.strip() if product.find('span', class_='money') else 'N/A'\n",
    "        img_tags = product.find_all('img')\n",
    "\n",
    "        # Handling multiple images\n",
    "        images_info = []\n",
    "        for img_tag in img_tags:\n",
    "            if img_tag and 'src' in img_tag.attrs:\n",
    "                img_url = f\"https:{img_tag['src']}\"\n",
    "                image_id = urlparse(img_url).path.split('/')[-1].split('.')[0]\n",
    "                downloaded_image_id, downloaded_image_url = download_image(img_url, image_directory, image_id)\n",
    "                if downloaded_image_id:\n",
    "                    images_info.append((downloaded_image_id, downloaded_image_url))\n",
    "\n",
    "        product_info = {\n",
    "            'name': name,\n",
    "            'price': price,\n",
    "            'url': product_url,\n",
    "            'images': images_info\n",
    "        }\n",
    "        products.append(product_info)\n",
    "\n",
    "    return products\n",
    "\n",
    "def scrape_and_save(urls, csv_file_path):\n",
    "    \"\"\" Scrape product data from URLs and save to a CSV file \"\"\"\n",
    "    with open(csv_file_path, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Product Name', 'Price', 'Color', 'Product Page URL', 'Downloaded Image ID', 'Downloaded Image URL'])\n",
    "\n",
    "        for url in urls:\n",
    "            page_number = 1\n",
    "            while True:\n",
    "                page_url = f\"{url}?page={page_number}\"\n",
    "                products = get_product_info(page_url)\n",
    "                if not products:\n",
    "                    break\n",
    "                for product in products:\n",
    "                    for image_id, image_url in product['images']:\n",
    "                        writer.writerow([product['name'], product['price'], 'N/A', product['url'], image_id, image_url])\n",
    "                page_number += 1\n",
    "                time.sleep(1)  # Respectful delay between requests\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    urls = [\n",
    "        f\"{base_url}/collections/man-t-shirts\",\n",
    "        f\"{base_url}/collections/man-sweaters-cardigans\",\n",
    "        f\"{base_url}/collections/man-shoes\",\n",
    "        f\"{base_url}/collections/man-pants\",\n",
    "        f\"{base_url}/collections/man-blazers\",\n",
    "        f\"{base_url}/collections/man-polo\",\n",
    "        f\"{base_url}/collections/man-shirts\",\n",
    "        f\"{base_url}/collections/man-jackets-coats\",\n",
    "        f\"{base_url}/collections/man-hoodies-sweatshirt\"\n",
    "    ]\n",
    "    scrape_and_save(urls, csv_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get prod name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 210\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    199\u001b[0m     urls \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    200\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/collections/man-t-shirts\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/collections/man-sweaters-cardigans\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/collections/man-hoodies-sweatshirt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m     ]\n\u001b[0;32m--> 210\u001b[0m     \u001b[43mscrape_and_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43murls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_file_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 189\u001b[0m, in \u001b[0;36mscrape_and_save\u001b[0;34m(urls, csv_file_path)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    188\u001b[0m     page_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m?page=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 189\u001b[0m     products \u001b[38;5;241m=\u001b[39m \u001b[43mget_product_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m products:\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 150\u001b[0m, in \u001b[0;36mget_product_info\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Extract product details from a given URL \"\"\"\u001b[39;00m\n\u001b[1;32m    149\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[0;32m--> 150\u001b[0m soup \u001b[38;5;241m=\u001b[39m \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhtml.parser\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m products \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m product \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrid-product\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/bs4/__init__.py:335\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39minitialize_soup(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 335\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m     success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/bs4/__init__.py:478\u001b[0m, in \u001b[0;36mBeautifulSoup._feed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;66;03m# Convert the document to Unicode.\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m--> 478\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;66;03m# Close out any unfinished strings and close all the open tags.\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendData()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/bs4/builder/_htmlparser.py:380\u001b[0m, in \u001b[0;36mHTMLParserTreeBuilder.feed\u001b[0;34m(self, markup)\u001b[0m\n\u001b[1;32m    378\u001b[0m parser\u001b[38;5;241m.\u001b[39msoup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoup\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 380\u001b[0m     \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m     parser\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;66;03m# html.parser raises AssertionError in rare cases to\u001b[39;00m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;66;03m# indicate a fatal problem with the markup, especially\u001b[39;00m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;66;03m# when there's an error in the doctype declaration.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/html/parser.py:110\u001b[0m, in \u001b[0;36mHTMLParser.feed\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Feed data to the parser.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03mCall this as often as you want, with as little or as much text\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03mas you want (may include '\\n').\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrawdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrawdata \u001b[38;5;241m+\u001b[39m data\n\u001b[0;32m--> 110\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgoahead\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/html/parser.py:170\u001b[0m, in \u001b[0;36mHTMLParser.goahead\u001b[0;34m(self, end)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m startswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m, i):\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m starttagopen\u001b[38;5;241m.\u001b[39mmatch(rawdata, i): \u001b[38;5;66;03m# < + letter\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m         k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_starttag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m startswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</\u001b[39m\u001b[38;5;124m\"\u001b[39m, i):\n\u001b[1;32m    172\u001b[0m         k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_endtag(i)\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/html/parser.py:325\u001b[0m, in \u001b[0;36mHTMLParser.parse_starttag\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    323\u001b[0m     attrvalue \u001b[38;5;241m=\u001b[39m attrvalue[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attrvalue:\n\u001b[0;32m--> 325\u001b[0m     attrvalue \u001b[38;5;241m=\u001b[39m \u001b[43munescape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattrvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m attrs\u001b[38;5;241m.\u001b[39mappend((attrname\u001b[38;5;241m.\u001b[39mlower(), attrvalue))\n\u001b[1;32m    327\u001b[0m k \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/html/__init__.py:122\u001b[0m, in \u001b[0;36munescape\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m    118\u001b[0m _charref \u001b[38;5;241m=\u001b[39m _re\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&(#[0-9]+;?\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    119\u001b[0m                        \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m|#[xX][0-9a-fA-F]+;?\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m                        \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m|[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mf <&#;]\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m1,32};?)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munescape\u001b[39m(s):\n\u001b[1;32m    123\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    Convert all named and numeric character references (e.g. &gt;, &#62;,\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03m    &x3e;) in the string s to the corresponding unicode characters.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    HTML 5 named character references defined in html.entities.html5.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m s:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "product id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "from urllib.parse import urlparse, unquote\n",
    "import time\n",
    "import re  # Import regular expressions\n",
    "\n",
    "# Headers to mimic a browser request\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Base URL for product pages\n",
    "base_url = \"https://lamaretail.com\"\n",
    "\n",
    "# Directory to save images\n",
    "image_directory = 'LamaRetail_images'\n",
    "os.makedirs(image_directory, exist_ok=True)\n",
    "\n",
    "# CSV file setup\n",
    "csv_file_path = 'lama_retail_products.csv'\n",
    "\n",
    "def download_image(image_url, image_directory, product_id, index):\n",
    "    \"\"\" Download image and save locally \"\"\"\n",
    "    image_filename = f\"{product_id}_{index}.jpg\"\n",
    "    image_path = os.path.join(image_directory, image_filename)\n",
    "    response = requests.get(image_url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        with open(image_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        return image_filename, image_url\n",
    "    return None, None\n",
    "\n",
    "def extract_product_id_from_url(url):\n",
    "    \"\"\" Extract the product ID from the product URL \"\"\"\n",
    "    # Capture everything after 'products/' up to a potential query or hash\n",
    "    match = re.search(r'/products/([^/?#]+)', url)\n",
    "    if match:\n",
    "        product_id = match.group(1).replace('-', '').upper()  # Replace hyphens and convert to uppercase\n",
    "        return product_id\n",
    "    return 'N/A'\n",
    "\n",
    "def get_product_info(url):\n",
    "    \"\"\" Extract product details from a given URL \"\"\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    products = []\n",
    "\n",
    "    for product in soup.find_all('div', class_='grid-product'):\n",
    "        product_url = base_url + product.find('a')['href']\n",
    "        product_id = extract_product_id_from_url(product_url)  # Use the new function to extract the product ID\n",
    "        name = product.find('div', class_='grid-product__title').text.strip() if product.find('div', class_='grid-product__title') else 'N/A'\n",
    "        price = product.find('span', class_='money').text.strip() if product.find('span', class_='money') else 'N/A'\n",
    "        img_tags = product.find_all('img')\n",
    "\n",
    "        # Handling multiple images\n",
    "        images_info = []\n",
    "        for index, img_tag in enumerate(img_tags, start=1):\n",
    "            if img_tag and 'src' in img_tag.attrs:\n",
    "                img_url = f\"https:{img_tag['src']}\"\n",
    "                downloaded_image_id, downloaded_image_url = download_image(img_url, image_directory, product_id, index)\n",
    "                if downloaded_image_id:\n",
    "                    images_info.append((downloaded_image_id, downloaded_image_url))\n",
    "\n",
    "        product_info = {\n",
    "            'product_id': product_id,\n",
    "            'name': name,\n",
    "            'price': price,\n",
    "            'url': product_url,\n",
    "            'images': images_info\n",
    "        }\n",
    "        products.append(product_info)\n",
    "\n",
    "    return products\n",
    "\n",
    "def scrape_and_save(urls, csv_file_path):\n",
    "    \"\"\" Scrape product data from URLs and save to a CSV file \"\"\"\n",
    "    with open(csv_file_path, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Product ID', 'Product Name', 'Price', 'Color', 'Product Page URL', 'Downloaded Image ID', 'Downloaded Image URL'])\n",
    "\n",
    "        for url in urls:\n",
    "            page_number = 1\n",
    "            while True:\n",
    "                page_url = f\"{url}?page={page_number}\"\n",
    "                products = get_product_info(page_url)\n",
    "                if not products:\n",
    "                    break\n",
    "                for product in products:\n",
    "                    for image_id, image_url in product['images']:\n",
    "                        writer.writerow([product['product_id'], product['name'], product['price'], 'N/A', product['url'], image_id, image_url])\n",
    "                page_number += 1\n",
    "                time.sleep(1)  # Respectful delay between requests\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    urls = [\n",
    "        f\"{base_url}/collections/man-t-shirts\",\n",
    "        f\"{base_url}/collections/man-sweaters-cardigans\",\n",
    "        f\"{base_url}/collections/man-shoes\",\n",
    "        f\"{base_url}/collections/man-pants\",\n",
    "        f\"{base_url}/collections/man-blazers\",\n",
    "        f\"{base_url}/collections/man-polo\",\n",
    "        f\"{base_url}/collections/man-shirts\",\n",
    "        f\"{base_url}/collections/man-jackets-coats\",\n",
    "        f\"{base_url}/collections/man-hoodies-sweatshirt\"\n",
    "    ]\n",
    "    scrape_and_save(urls, csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "from urllib.parse import urlparse, unquote\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Headers to mimic a browser request\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Base URL for product pages\n",
    "base_url = \"https://lamaretail.com\"\n",
    "\n",
    "# Directory to save images\n",
    "image_directory = 'LamaRetail_images'\n",
    "os.makedirs(image_directory, exist_ok=True)\n",
    "\n",
    "# CSV file setup\n",
    "csv_file_path = 'lama_retail_products.csv'\n",
    "\n",
    "def download_image(image_url, image_directory, product_id, index):\n",
    "    \"\"\" Download image and save locally \"\"\"\n",
    "    image_filename = f\"{product_id}_{index}.jpg\"\n",
    "    image_path = os.path.join(image_directory, image_filename)\n",
    "    response = requests.get(image_url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        with open(image_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        return image_filename, image_url\n",
    "    return None, None\n",
    "\n",
    "def extract_product_id_from_url(url):\n",
    "    \"\"\" Extract the product ID from the product URL \"\"\"\n",
    "    match = re.search(r'/products/(.+)', url)\n",
    "    if match:\n",
    "        return match.group(1)  # Return the full product ID part of the URL\n",
    "    return 'N/A'\n",
    "\n",
    "def parse_product_name_from_url(product_id):\n",
    "    \"\"\" Convert hyphenated product ID to a more readable product name \"\"\"\n",
    "    name_parts = product_id.split('-')[:-1]  # Exclude the last part if it seems like a code or color\n",
    "    return ' '.join(part.capitalize() for part in name_parts)\n",
    "\n",
    "def get_product_info(url):\n",
    "    \"\"\" Extract product details from a given URL \"\"\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    products = []\n",
    "\n",
    "    for product in soup.find_all('div', class_='grid-product'):\n",
    "        product_url = base_url + product.find('a')['href']\n",
    "        product_id = extract_product_id_from_url(product_url)\n",
    "        name = parse_product_name_from_url(product_id)\n",
    "        price = product.find('span', class_='money').text.strip() if product.find('span', class_='money') else 'N/A'\n",
    "        img_tags = product.find_all('img')\n",
    "\n",
    "        # Handling multiple images\n",
    "        images_info = []\n",
    "        for index, img_tag in enumerate(img_tags, start=1):\n",
    "            if img_tag and 'src' in img_tag.attrs:\n",
    "                img_url = f\"https:{img_tag['src']}\"\n",
    "                downloaded_image_id, downloaded_image_url = download_image(img_url, image_directory, product_id, index)\n",
    "                if downloaded_image_id:\n",
    "                    images_info.append((downloaded_image_id, downloaded_image_url))\n",
    "\n",
    "        product_info = {\n",
    "            'product_id': product_id,\n",
    "            'name': name,\n",
    "            'price': price,\n",
    "            'url': product_url,\n",
    "            'images': images_info\n",
    "        }\n",
    "        products.append(product_info)\n",
    "\n",
    "    return products\n",
    "\n",
    "def scrape_and_save(urls, csv_file_path):\n",
    "    \"\"\" Scrape product data from URLs and save to a CSV file \"\"\"\n",
    "    with open(csv_file_path, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Product ID', 'Product Name', 'Price', 'Color', 'Product Page URL', 'Downloaded Image ID', 'Downloaded Image URL'])\n",
    "\n",
    "        for url in urls:\n",
    "            page_number = 1\n",
    "            while True:\n",
    "                page_url = f\"{url}?page={page_number}\"\n",
    "                products = get_product_info(page_url)\n",
    "                if not products:\n",
    "                    break\n",
    "                for product in products:\n",
    "                    for image_id, image_url in product['images']:\n",
    "                        writer.writerow([product['product_id'], product['name'], product['price'], 'N/A', product['url'], image_id, image_url])\n",
    "                page_number += 1\n",
    "                time.sleep(1)  # Respectful delay between requests\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    urls = [\n",
    "        f\"{base_url}/collections/man-t-shirts\",\n",
    "        f\"{base_url}/collections/man-sweaters-cardigans\",\n",
    "        f\"{base_url}/collections/man-shoes\",\n",
    "        f\"{base_url}/collections/man-pants\",\n",
    "        f\"{base_url}/collections/man-blazers\",\n",
    "        f\"{base_url}/collections/man-polo\",\n",
    "        f\"{base_url}/collections/man-shirts\",\n",
    "        f\"{base_url}/collections/man-jackets-coats\",\n",
    "        f\"{base_url}/collections/man-hoodies-sweatshirt\"\n",
    "    ]\n",
    "    scrape_and_save(urls, csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
